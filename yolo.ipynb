{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8vgZZ-nDWJJ",
        "outputId": "5c682624-5c8e-42dd-d371-aed28d757480"
      },
      "outputs": [],
      "source": [
        "%pip install torch==1.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "%pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7RrjmVUEfib",
        "outputId": "2a2f0f0e-5103-44e4-ca2d-293d216495cb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torchvision version:\", torchvision.__version__)\n",
        "\n",
        "print(torch.cuda.get_device_properties(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-OrPuvRvjXF",
        "outputId": "06f5bfb5-d206-48e6-85d8-8f38fe126b69"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSnGYYPTvjXF"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/Colab/YOLO /content/YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Act-KZnrvjXG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/YOLO\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFozZaRhvjXH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VOCDetection\n",
        "from tqdm import tqdm\n",
        "\n",
        "import utils\n",
        "from model import YOLO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuGyKJqsvjXH",
        "outputId": "b137df3d-deb3-4f33-a9ee-65cd1ed18a23"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "phases = (\"train\", \"val\")\n",
        "\n",
        "\n",
        "class SquarePad:\n",
        "    def __call__(self, image):\n",
        "        max_wh = max(image.size)\n",
        "        p_left, p_top = [(max_wh - s) // 2 for s in image.size]\n",
        "        p_right, p_bottom = [\n",
        "            max_wh - (s + p) for s, p in zip(image.size, (p_left, p_top))\n",
        "        ]\n",
        "        padding = (p_left, p_top, p_right, p_bottom)\n",
        "        return transforms.functional.pad(image, padding, 0, \"constant\")\n",
        "\n",
        "\n",
        "img_height = 416\n",
        "img_width = 416\n",
        "\n",
        "\n",
        "def get_transform(phase):\n",
        "    transform_list = []\n",
        "    if phase == \"train\":\n",
        "        transform_list.append(transforms.ColorJitter(0.1, 0.1, 0.1, 0.1))\n",
        "    transform_list.append(SquarePad())\n",
        "    transform_list.append(transforms.Resize((img_height, img_width)))\n",
        "    transform_list.append(transforms.ToTensor())\n",
        "    transform_list.append(\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    )\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "\n",
        "transform = {phase: get_transform(phase) for phase in phases}\n",
        "\n",
        "dataset = {\n",
        "    phase: VOCDetection(\n",
        "        \"./data\",\n",
        "        image_set=phase,\n",
        "        download=True,\n",
        "        transform=transform[phase],\n",
        "    )\n",
        "    for phase in phases\n",
        "}\n",
        "\n",
        "\n",
        "def get_classes(class_path):\n",
        "    with open(class_path, \"rt\") as f:\n",
        "        cls = {line.strip() for line in f}\n",
        "    return dict(zip(cls, range(len(cls))))\n",
        "\n",
        "\n",
        "cls_idx = get_classes(\"./data/classes.txt\")\n",
        "num_classes = len(cls_idx)\n",
        "idx_cls = {v: k for k, v in cls_idx.items()}\n",
        "\n",
        "\n",
        "def collate_fn(sample):\n",
        "    images = []\n",
        "    max_num_box = max([len(each[1][\"annotation\"][\"object\"]) for each in sample])\n",
        "    boxes = torch.full((len(sample), max_num_box, 5), -1)\n",
        "    filenames = []\n",
        "    for i, (image, target) in enumerate(sample):\n",
        "        images.append(image)\n",
        "        width = int(target[\"annotation\"][\"size\"][\"width\"])\n",
        "        height = int(target[\"annotation\"][\"size\"][\"height\"])\n",
        "        max_wh = max(width, height)\n",
        "        x_offset = (max_wh - width) // 2\n",
        "        y_offset = (max_wh - height) // 2\n",
        "        for j, each in enumerate(target[\"annotation\"][\"object\"]):\n",
        "            cls = cls_idx[each[\"name\"]]\n",
        "            x_1 = (int(each[\"bndbox\"][\"xmin\"]) + x_offset) * img_width / max_wh\n",
        "            y_1 = (int(each[\"bndbox\"][\"ymin\"]) + y_offset) * img_height / max_wh\n",
        "            x_2 = (int(each[\"bndbox\"][\"xmax\"]) + x_offset) * img_width / max_wh\n",
        "            y_2 = (int(each[\"bndbox\"][\"ymax\"]) + y_offset) * img_height / max_wh\n",
        "            boxes[i][j] = torch.tensor((x_1, y_1, x_2, y_2, cls))\n",
        "        filenames.append(target[\"annotation\"][\"filename\"])\n",
        "    return torch.stack(images, 0), boxes, filenames\n",
        "\n",
        "\n",
        "dataloader = {\n",
        "    phase: DataLoader(\n",
        "        dataset[phase],\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        generator=torch.Generator(device),\n",
        "    )\n",
        "    for phase in phases\n",
        "}\n",
        "\n",
        "\n",
        "def get_anchors(anchor_path):\n",
        "    anchors = []\n",
        "    with open(anchor_path, \"rt\") as f:\n",
        "        for line in f:\n",
        "            anchors.extend([float(x) for x in line.strip().split(\",\")])\n",
        "    return torch.tensor(anchors).view(3, 3, 2)\n",
        "\n",
        "\n",
        "anchors = get_anchors(\"./data/anchors.txt\")\n",
        "num_anchors = anchors.size(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuNNz1jYvjXK"
      },
      "outputs": [],
      "source": [
        "lr = 1e-2\n",
        "epochs = 100\n",
        "swa_start = epochs - 30\n",
        "\n",
        "checkpoint = torch.load(\"./data/checkpoint.pt\", map_location=device)\n",
        "\n",
        "model = YOLO(num_anchors, num_classes).to(device)\n",
        "model.load_state_dict(checkpoint[\"model\"])\n",
        "\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "params = [param for param in model.parameters() if param.requires_grad]\n",
        "\n",
        "optimizer = torch.optim.AdamW(params, lr=lr, amsgrad=False)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=lr,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=len(dataloader[\"train\"]),\n",
        "    three_phase=True,\n",
        ")\n",
        "scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "\n",
        "ema_avg = (\n",
        "    lambda averaged_model_parameter, model_parameter, num_averaged: 0.1\n",
        "    * averaged_model_parameter\n",
        "    + 0.9 * model_parameter\n",
        ")\n",
        "swa_model = torch.optim.swa_utils.AveragedModel(model, device=device, avg_fn=ema_avg)\n",
        "swa_model.load_state_dict(checkpoint[\"swa_model\"])\n",
        "\n",
        "swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=lr)\n",
        "swa_scheduler.load_state_dict(checkpoint[\"swa_scheduler\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJQLwvMkvjXK",
        "outputId": "bcb18ca7-b327-4044-b09b-144fea81ec99"
      },
      "outputs": [],
      "source": [
        "loss_hist = checkpoint[\"loss\"]\n",
        "for epoch in range(checkpoint[\"epoch\"] + 1, epochs):\n",
        "    for phase in phases:\n",
        "        if phase == \"train\":\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        with torch.set_grad_enabled(phase == \"train\"):\n",
        "            for images, boxes, _ in tqdm(\n",
        "                dataloader[phase], desc=f\"{phase} [{epoch}/{epochs}]\"\n",
        "            ):\n",
        "                images = images.to(device)\n",
        "                boxes = boxes.to(device)\n",
        "                prdictions = model(images)\n",
        "\n",
        "                _, loss = utils.decode_predictions(anchors, prdictions, boxes)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if epoch < swa_start:\n",
        "                        scheduler.step()\n",
        "\n",
        "            if phase == \"train\" and epoch >= swa_start:\n",
        "                swa_model.update_parameters(model)\n",
        "                swa_scheduler.step()\n",
        "\n",
        "        running_loss /= len(dataloader[phase])\n",
        "        loss_hist[phase].append(running_loss)\n",
        "\n",
        "        if phase == \"val\":\n",
        "            if epoch == epochs - 1:\n",
        "                torch.optim.swa_utils.update_bn(dataloader[\"train\"], swa_model)\n",
        "\n",
        "            x = range(len(loss_hist[\"train\"]))\n",
        "            plt.plot(x, loss_hist[\"train\"], label=\"train\")\n",
        "            plt.plot(x, loss_hist[\"val\"], label=\"val\")\n",
        "            plt.legend()\n",
        "            plt.savefig(\"/content/drive/MyDrive/Colab/YOLO/data/loss_hist.png\")\n",
        "\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model\": model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scheduler\": scheduler.state_dict(),\n",
        "                    \"swa_model\": swa_model.state_dict(),\n",
        "                    \"swa_scheduler\": swa_scheduler.state_dict(),\n",
        "                    \"loss\": loss_hist,\n",
        "                },\n",
        "                \"/content/drive/MyDrive/Colab/YOLO/data/checkpoint.pt\",\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "# os.chdir('/home/zli/cv/yolo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "file = \"./street.jpg\"\n",
        "img = cv2.imread(file)\n",
        "cv2.imshow('img', img)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# class SquarePad:\n",
        "#     def __call__(self, image):\n",
        "#         max_wh = max(image.size)\n",
        "#         p_left, p_top = [(max_wh - s) // 2 for s in image.size]\n",
        "#         p_right, p_bottom = [\n",
        "#             max_wh - (s + p) for s, p in zip(image.size, (p_left, p_top))\n",
        "#         ]\n",
        "#         padding = (p_left, p_top, p_right, p_bottom)\n",
        "#         return transforms.functional.pad(image, padding, 0, \"constant\")\n",
        "# img = transforms.Compose(\n",
        "#     (\n",
        "#         SquarePad(),\n",
        "#         transforms.Resize((img_height, img_width)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "#     )\n",
        "# )(img)\n",
        "# print(img.size())\n",
        "\n",
        "\n",
        "# checkpoint = torch.load(\"./data/checkpoint.pt\", map_location=device)\n",
        "\n",
        "# model = YOLO(num_anchors, num_classes).to(device)\n",
        "# model.load_state_dict(checkpoint[\"model\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "howqdbOKJNDg"
      },
      "outputs": [],
      "source": [
        "metric = MeanAveragePrecision()\n",
        "preds = []\n",
        "targets = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X, y, _ in tqdm(dataloader[\"val\"]):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        result, loss = utils.decode_predictions(anchors, pred, y)\n",
        "        outputs = utils.postprocess_result(result)\n",
        "        for i, (b, s, l) in enumerate(outputs):\n",
        "            preds.append({\"boxes\": b, \"scores\": s, \"labels\": l})\n",
        "            targets.append({\"boxes\": y[i][:, :4], \"labels\": y[i][:, -1]})\n",
        "metric.update(preds, targets)\n",
        "for k, v in metric.compute():\n",
        "    print(f\"{k}: {v.item()}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "yolo.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "4daa7ebe6f85a444fc01d83e5620d0614d04c73dabdaccdac45a4d48c0e2c05f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
